# -*- coding: utf-8 -*-
"""Public-SafariOnline-Day2-Part1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/github/noahgift/functional_intro_to_python/blob/master/Public_SafariOnline_Day2_Part1.ipynb

# Essential Machine Learning and Exploratory Data Analysis with Python and Jupyter Notebook

## Pragmatic AI Labs
![alt text](https://paiml.com/images/logo_with_slogan_white_background.png)

This notebook was produced by [Pragmatic AI Labs](https://paiml.com/).  You can continue learning about these topics by:

*   Buying a copy of [Pragmatic AI: An Introduction to Cloud-Based Machine Learning](http://www.informit.com/store/pragmatic-ai-an-introduction-to-cloud-based-machine-9780134863917)
*   Reading an online copy of [Pragmatic AI:Pragmatic AI: An Introduction to Cloud-Based Machine Learning](https://www.safaribooksonline.com/library/view/pragmatic-ai-an/9780134863924/)
*  Watching video [Essential Machine Learning and AI with Python and Jupyter Notebook-Video-SafariOnline](https://www.safaribooksonline.com/videos/essential-machine-learning/9780135261118) on Safari Books Online.
* Watching video [AWS Certified Machine Learning-Speciality](https://learning.oreilly.com/videos/aws-certified-machine/9780135556597)
* Purchasing video [Essential Machine Learning and AI with Python and Jupyter Notebook- Purchase Video](http://www.informit.com/store/essential-machine-learning-and-ai-with-python-and-jupyter-9780135261095)
*   Viewing more content at [noahgift.com](https://noahgift.com/)

### Additional Related Topics from Noah Gift

His most recent books are:

*   [Pragmatic A.I.:   An introduction to Cloud-Based Machine Learning (Pearson, 2018)](https://www.amazon.com/Pragmatic-AI-Introduction-Cloud-Based-Analytics/dp/0134863860)
*   [Python for DevOps (O'Reilly, 2020)](https://www.amazon.com/Python-DevOps-Ruthlessly-Effective-Automation/dp/149205769X). 

His most recent video courses are:

*   [Essential Machine Learning and A.I. with Python and Jupyter Notebook LiveLessons (Pearson, 2018)](https://learning.oreilly.com/videos/essential-machine-learning/9780135261118)
*   [AWS Certified Machine Learning-Specialty (ML-S) (Pearson, 2019)](https://learning.oreilly.com/videos/aws-certified-machine/9780135556597)
*   [Python for Data Science Complete Video Course Video Training (Pearson, 2019)](https://learning.oreilly.com/videos/python-for-data/9780135687253)
*   [AWS Certified Big Data - Specialty Complete Video Course and Practice Test Video Training (Pearson, 2019)](https://learning.oreilly.com/videos/aws-certified-big/9780135772324)
*   [Building A.I. Applications on Google Cloud Platform (Pearson, 2019)](https://learning.oreilly.com/videos/building-ai-applications/9780135973462)
*   [Pragmatic AI and Machine Learning Core Principles (Pearson, 2019)](https://learning.oreilly.com/videos/pragmatic-ai-and/9780136554714)
*   [Data Engineering with Python and AWS Lambda (Pearson, 2019)](https://learning.oreilly.com/videos/data-engineering-with/9780135964330)

His most recent online courses are:

*   [Microservices with this Udacity DevOps Nanodegree (Udacity, 2019)](https://www.udacity.com/course/cloud-dev-ops-nanodegree--nd9991)
*   [Command Line Automation in Python (DataCamp, 2019)](https://www.datacamp.com/instructors/ndgift)

# Part 2.1: IO Operations in Python and Pandas and Data Science Project Exploration

* *[Watch Video Lesson 6:  IO Operations in Python and Pandas](https://www.safaribooksonline.com/videos/essential-machine-learning/9780135261118/9780135261118-EMLA_01_06_00)*

## Working with Files

### Writing to a file

* *[Watch Video Lesson 6.1:  Use write file operations](https://www.safaribooksonline.com/videos/essential-machine-learning/9780135261118/9780135261118-EMLA_01_06_01)*
"""

f = open('workfile.txt', 'w')
f.write("foo20\n")
f.write("foo3\n")
f.write("foo4\n")
f.close()
!cat workfile.txt

#!ls -l

my_writes = ["foo2\n","foo3\n","foo4\n"]
f = open('workfile2.txt', 'w')
for line in my_writes:
  f.write(line)
f.close()
!cat workfile2.txt

"""#### Writing to a file with 'context'"""

with open("workfile.txt", "w") as workfile:
    workfile.write("bam")
!cat workfile.txt

"""### Reading a file in

* *[Watch Video Lesson 6.2:  Use read file operations](https://www.safaribooksonline.com/videos/essential-machine-learning/9780135261118/9780135261118-EMLA_01_06_02)*
"""

f = open("workfile.txt", "r")
out = f.readlines() #r.read() works as well
# Maybe we want to create a generator pipeline
for line in out:
  print(line)
  #  yield line
f.close()
print(out)

"""#### Reading a file with 'context'"""

with open("workfile.txt", "r") as workfile:
    print(workfile.readlines())
    #print(workfile.read())

"""## Serialization Techniques

* *[Watch Video Lesson 6.3:  Use serialization techniques](https://www.safaribooksonline.com/videos/essential-machine-learning/9780135261118/9780135261118-EMLA_01_06_03)*

### Serialize a Python Dictionary to Pickle
"""

mydict = {"one":1, "two":2}

import pickle

pickle.dump(mydict, open('mydictionary.pickle', 'wb'))

!ls -l mydictionary.pickle

#!cat mydictionary.pickle

res = pickle.load(open('mydictionary.pickle', "rb"))

print(res)

"""### Serialize a Python Dictionary to JSON"""

import json
with open('data.json', 'w') as outfile:
    json.dump(res, outfile)

!cat data.json

with open('data.json', 'rb') as outfile:
    res2 = json.load(outfile)

print(res2)
type(res2)

"""### Save to Yaml"""

import yaml

with open("data.yaml", "w") as yamlfile:                                               
    yaml.safe_dump(res2, yamlfile, default_flow_style=False)

!cat data.yaml

"""### Load Yaml"""

with open("data.yaml", "rb") as yamlfile:                                               
    res3 = yaml.safe_load(yamlfile)

print(res3)
type(res3)

"""## Use Pandas DataFrames

* *[Watch Video Lesson 6.4:  Use Pandas DataFrames](https://www.safaribooksonline.com/videos/essential-machine-learning/9780135261118/9780135261118-EMLA_01_06_04)*

#### Creating Pandas DataFrames

##### Creating DataFrames CSV file



*   Can be local
*   Can be hosted on a website
"""

import pandas as pd
csv_url = 'https://raw.githubusercontent.com/noahgift/mma/master/data/ufc_fights_all.csv'
mma_df = pd.read_csv(csv_url)
mma_df.head(2)

mma_df.shape

mma_df.to_json()

mma_df.describe()

"""##### List to Pandas DataFrame 

Convert a list to Pandas DataFrame
"""

new_df = pd.DataFrame(["one", "two", "three"])
new_df.columns = ["numbers"]
new_df.head()

new_df.index

#new column
new_df["words"] = ["one", "two", "three"]
new_df.head()

#Subset
winning_technique_list = mma_df['method_d'].tolist()
techniques_df = pd.DataFrame(winning_technique_list)
techniques_df.columns = ["winning_technique"]
techniques_df.head(3)

# Verify what you did
techniques_df.shape

techniques_df.describe()

"""#### Exporting Pandas DataFrames

##### Pandas DataFrame Column to List

Can use "tolist"
"""

winning_technique_list = mma_df['method_d'].tolist()
new_vals = winning_technique_list[-2:]
for val in new_vals:
  print(val)

"""##### Pandas DataFrame to Dictionary

Grab a couple of records and convert to Python dictionary
"""

#mma_dict = mma_df.head(2).to_dict()
mma_dict = mma_df.tail(2).to_dict()
#mma_dict
columns = list(mma_dict.keys())
#print(f"These are the columns in Pandas: {columns}")
mma_dict.values()

mma_dict

"""##### Dictionary to Pandas DataFrame

Take dictionary full of dictionaries and make it a Pandas DataFrame
"""

mma_df_new = pd.DataFrame(mma_dict)
mma_df_new.head()

"""##### Pandas DataFrame to CSV

Write out DataFrame using to_csv
"""

mma_df.head().to_csv("small_mma_records.csv")
#mma_df.to_csv("large_mma_records.csv") #entire dataframe as csv
!cat small_mma_records.csv



"""#### Using Pandas on Ray

More info on Pandas:  https://rise.cs.berkeley.edu/blog/pandas-on-ray/

*Note:  Pandas is small data...data science.  You may need to use Spark on Pandas on Ray

#### Using Pandas on Dask

Dask natively scales Python

https://dask.org/

#### Using Google Sheets with Pandas DataFrames

Reference:  [Official Google Colab Documentation on IO](https://colab.research.google.com/notebooks/io.ipynb)

**Install Google Spreadsheet Library**
"""

!pip install --upgrade -q gspread

"""**Authenticate to API**"""

from google.colab import auth
auth.authenticate_user()

import gspread
from oauth2client.client import GoogleCredentials

gc = gspread.authorize(GoogleCredentials.get_application_default())

"""**Create a Spreadsheet and Put Items in It**

Note, could use existing spreadsheet
"""

sh = gc.create('pragmaticai-test')
worksheet = gc.open('pragmaticai-test').sheet1
cell_list = worksheet.range('A1:A10')

import random
count = 0
for cell in cell_list:
  count +=1
  cell.value = count
worksheet.update_cells(cell_list)

"""**Convert Spreadsheet Data to Pandas DataFrame**"""

worksheet = gc.open('pragmaticai-test').sheet1
rows = worksheet.get_all_values()
import pandas as pd
df = pd.DataFrame.from_records(rows)
print(df.median())
df

#df.head()

df.describe()

"""## Concurrency in Python

* *[Watch Video Lesson 6.6:  Use concurrency methods in Python](https://www.safaribooksonline.com/videos/essential-machine-learning/9780135261118/9780135261118-EMLA_01_06_06)*

### Threads

Threads are the beatup Pinto of concurrency in Python.  They lack the ability to scale to multiple cores and often cause performance problems.  Almost always you should choose some other method of concurrency in Python.

*Typically they are used in situations where things are IO bound, not CPU bound.*

![Pinto](https://homeprohub.files.wordpress.com/2013/03/cost-of-window-replacement.jpg)
"""



"""##### Simple Threading Example"""

import threading

def fight_club(x):
  
    print(f"Processing Thread# {num}: Calculating punch with attack strength {x} to the {x} power\n")
    return x**x
  
workers = []
for num in range(1,6):
  print(f"Queuing thread # {num}\n")
  thread = threading.Thread(target=fight_club, args=(num,))
  workers.append(thread)
  thread.start()

"""#### Using the subprocess command

A general purpose way to "Shell Out" to system commands
"""

import subprocess
#res = subprocess.Popen("ls -l", shell=True, stdout=subprocess.PIPE) #not ideal
res = subprocess.Popen(["ls", "-l"], stdout=subprocess.PIPE)
out = res.stdout.readlines()
print(out)
!ls -l

#also worth noting in 3.7 subprocess.run(console_output=True)
!python3 --version

!ls -l

"""### Multiprocessing

#### Mapping processes to Functions

Processes are forked and run truly parallel (unlike threads)
"""

from multiprocessing import Pool
import datetime
import time
import random

def fight_club(x):
  
    sleep_time = random.randrange(0,3)
    time.sleep(sleep_time)
    timestamp = datetime.datetime.now()
    print(f"Calculating punch with attack strength {x} to the {x} power: @timestamp {timestamp} with sleep {sleep_time}")
    return x**x

if __name__ == '__main__':
    p = Pool(5)
    print(p.map(fight_club, [1, 2, 3, 10, 100]))

from multiprocessing import cpu_count
cpu_count()

"""#### Process Pool Joined on Queue (Threadlike behavior)

Mimicks Threading interface, but with actual multi-core functionality
"""

from multiprocessing import Process, Queue

def f(q):
    q.put(["armbar", "kimura",  "Mata Leão"])

if __name__ == '__main__':
    q = Queue()
    p = Process(target=f, args=(q,))
    p.start()
    print(f"Grabbing some attacks: {q.get()}")    
    p.join()

"""### Async IO

#### Async IO in Python Examples

More info here:  https://docs.python.org/3/library/asyncio.html

**Using Python3 Async**

```python
import asyncio

def send_async_firehose_events(count=100):
    '''Async sends events to firehose'''

    start = time.time() 
    client = firehose_client()
    extra_msg = {"aws_service": "firehose"}
    loop = asyncio.get_event_loop()
    tasks = []
    LOG.info(f"sending aysnc events TOTAL {count}",extra=extra_msg)
    num = 0
    for _ in range(count):
        tasks.append(asyncio.ensure_future(put_record(gen_uuid_events(), client)))
        LOG.info(f"sending aysnc events: COUNT {num}/{count}")
        num +=1
    loop.run_until_complete(asyncio.wait(tasks))
    loop.close()
    end = time.time()  
    LOG.info("Total time: {}".format(end - start))
  ```

**Using trollius library with Python 2:  DEPRECATED**

```python
'''Generates an Async MetaData call.  Note, this isn't available in Boto3
In [56]: res = all_metadata_async()
In [57]: res
Out[57]: 
[('ami-manifest-path', <Response [200]>),
 ('instance-type', <Response [200]>),
 ('instance-id', <Response [200]>),
 ('iam', <Response [200]>),
 ('local-hostname', <Response [200]>),
 ('network', <Response [200]>),
 ('hostname', <Response [200]>),
 ('ami-id', <Response [200]>),
 ('instance-action', <Response [200]>),
 ('profile', <Response [200]>),
 ('reservation-id', <Response [200]>),
 ('security-groups', <Response [200]>),
 ('metrics', <Response [200]>),
 ('mac', <Response [200]>),
 ('public-ipv4', <Response [200]>),
 ('services', <Response [200]>),
 ('local-ipv4', <Response [200]>),
 ('placement', <Response [200]>),
 ('ami-launch-index', <Response [200]>),
 ('public-hostname', <Response [200]>),
 ('public-keys', <Response [200]>),
 ('block-device-mapping', <Response [200]>)]
'''

import requests
import trollius

def get_metadata_api_urls():
    '''Retrieves the api endpoints for metadata'''

    full_urls = {}
    metadata_url = "http://169.254.169.254/latest/meta-data/"
    resp = requests.get(metadata_url)
    urls = resp.content.split()
    for url in urls:
        stripped_url = url.rstrip("/")
        full_urls[stripped_url]=(os.path.join(metadata_url, url))
    return full_urls

def _get(key_url):
    key,url = key_url
    return key, requests.get(url)

def _do_calls(urls):
    loop = trollius.get_event_loop()
    futures = []
    for url in urls:
        futures.append(loop.run_in_executor(None, _get, url))
    return futures

@trollius.coroutine
def call():
    results = []
    futures = _do_calls(get_metadata_api_urls().items())
    for future in futures:
        result = yield trollius.From(future)
        results.append(result)
    raise trollius.Return(results)

def all_metadata_async():
    '''Retrieves all available metadata for an instance async'''

    loop = trollius.get_event_loop()
    res = loop.run_until_complete(call())
   ```
"""



"""### Serverless or FaaS (Functions as a service)

#### AWS Lambda

#####  AWS Lambda and Chalice Example

Standalone Lambda with Chalice:  http://chalice.readthedocs.io/en/latest/

```python
@app.lambda_function()
def send_message(event, context):
    '''Send a message to a channel'''

    slack_client = SlackClient(SLACK_TOKEN)
    res = slack_client.api_call(
      "chat.postMessage",
      channel="#general",
      text=event
    )
    return res
```

#### Fn Project

##### Fn Project

![Fn Project](https://camo.githubusercontent.com/aad13cfe0e267f38143fd8cc6816ab8adde37a56/687474703a2f2f666e70726f6a6563742e696f2f696d616765732f666e2d333030783132352e706e67)


*   [FN Project](https://fnproject.io/)
*   [FN Project Python Example](http://fnproject.io/tutorials/python/intro/)



```bash

fn init --runtime python --trigger http pythonfn

```



```python

import fdk
import json


def handler(ctx, data=None, loop=None):
    name = "World"
    if data and len(data) > 0:
        body = json.loads(data)
        name = body.get("name")
    return {"message": "Hello {0}".format(name)}



if __name__ == "__main__":
    fdk.handle(handler)
```

### Large Scale Concurrency Solutions

#### Larger Scale Concurrency



*   [AWS Step Functions with Lambda](https://aws.amazon.com/step-functions/)

![alt text](https://d1.awsstatic.com/product-marketing/Step%20Functions/OrderFullScreen.0e74c2f19d89a9325addb5bd746cd895b2e4c9c2.jpg)

*   [AWS Batch](https://aws.amazon.com/batch/)
![alt text](https://d1.awsstatic.com/Test%20Images/Kate%20Test%20Images/Dilithium_flowchart%20diagrams_v3_kw-02.322877d73eda8ed71a44db216a1d195550befac0.png)

*   [RabbitMQ Worker Farms-IBM Developerworks Article](https://www.ibm.com/developerworks/cloud/library/cl-optimizepythoncloud1/index.html)

![alt text](https://www.ibm.com/developerworks/cloud/library/cl-optimizepythoncloud2/figure1.gif)

### High Level Concurrency Overview for Machine Learning and HPC (High Performance Computing)

#### Diagram of Python Performance Problems


![63,000X Speedup for Matrix Multiply from Standard Python](https://user-images.githubusercontent.com/58792/45932870-37339000-bf38-11e8-8272-bf2addf56df1.png)

Source:  [Dave Patterson, UC Berkeley](https://www2.eecs.berkeley.edu/Faculty/Homepages/patterson.html)

#### Numba

[Numba](http://numba.pydata.org/)

*   open source JIT (Just in Time Compiler)
*   translates a subset of Python and Numpy code into fast machine code
*   Can approach speed of C
*   Can also parallize:  "true threads" and "GPU"

##### Install Numba
"""

!apt-get install nvidia-cuda-toolkit
!pip3 install numba

import os
os.environ['NUMBAPRO_LIBDEVICE'] = "/usr/lib/nvidia-cuda-toolkit/libdevice"
os.environ['NUMBAPRO_NVVM'] = "/usr/lib/x86_64-linux-gnu/libnvvm.so"

"""##### Use Numba"""

from numba import (cuda, vectorize)
import numba
import pandas as pd
import numpy as np

def real_estate_df():
    """30 Years of Housing Prices"""

    df = pd.read_csv("https://raw.githubusercontent.com/noahgift/real_estate_ml/master/data/Zip_Zhvi_SingleFamilyResidence.csv")
    df.rename(columns={"RegionName":"ZipCode"}, inplace=True)
    df["ZipCode"]=df["ZipCode"].map(lambda x: "{:.0f}".format(x))
    df["RegionID"]=df["RegionID"].map(lambda x: "{:.0f}".format(x))
    return df

def numerical_real_estate_array(df):
    """Converts df to numpy numerical array"""

    columns_to_drop = ['RegionID', 'ZipCode', 'City', 'State', 'Metro', 'CountyName']
    df_numerical = df.dropna()
    df_numerical = df_numerical.drop(columns_to_drop, axis=1)
    return df_numerical.values

def real_estate_array():
    """Returns Real Estate Array"""

    df = real_estate_df()
    rea = numerical_real_estate_array(df)
    return np.float32(rea)
  
rea = real_estate_array()

"""##### Use Numba decorator"""

@numba.jit(nopython=True)
def expmean_jit(rea):
    """Perform multiple mean calculations"""

    val = rea.mean() ** 2
    return val
  
expmean_jit(rea)

"""##### Multi-threaded numba

True multi-threaded code (Warning will use all cores on anymachine that runs it)
"""

@numba.jit(parallel=True)
def add_sum_threaded(rea):
    """Use all the cores"""

    x,_ = rea.shape
    total = 0
    for _ in numba.prange(x):
        total += rea.sum()  
        print(total)
        
add_sum_threaded(rea)

"""#### GPU

Heavily used in Deep Learning

*   NVidia
  - Numba [CUDA GPU ](http://numba.pydata.org/numba-doc/latest/cuda/index.html)
*  AMD
  - Numba [AMD ROC GPU](http://numba.pydata.org/numba-doc/latest/roc/index.html)

##### Use GPU
"""

@vectorize(['float32(float32, float32)'], target='cuda')
def add_ufunc(x, y):
    return x + y
  
def cuda_operation():
    """Performs Vectorized Operations on GPU"""

    x = real_estate_array()
    y = real_estate_array()

    print("Moving calculations to GPU memory")
    x_device = cuda.to_device(x)
    y_device = cuda.to_device(y)
    out_device = cuda.device_array(
        shape=(x_device.shape[0],x_device.shape[1]), dtype=np.float32)
    print(x_device)
    print(x_device.shape)
    print(x_device.dtype)

    print("Calculating on GPU")
    add_ufunc(x_device,y_device, out=out_device)

    out_host = out_device.copy_to_host()
    print(f"Calculations from GPU {out_host}")
    
cuda_operation()

"""#### TPU

[Tensor Processing Unit](https://cloud.google.com/tpu/docs/tpus)



*   "Google’s custom-developed application-specific integrated circuits (ASICs) used to accelerate machine learning workloads"
*   Available both in colab notebooks and on Google Cloud

## Walking through Social Power NBA Data Science Project

* *[Read related material covered in Chapter 6 of Pragmatic AI](https://www.safaribooksonline.com/library/view/pragmatic-ai-an/9780134863924/ch06.xhtml#ch06)*

* *[Watch Video Lesson 9:  Walking through Social Power NBA EDA and ML Project](https://www.safaribooksonline.com/videos/essential-machine-learning/9780135261118/9780135261118-EMLA_01_09_00)*

**Topics Covered**


* Data Collection Sources
* Importing and merging DataFrames in Pandas 
* Creating correlation heatmaps 
* Using seaborn lmplot 
* Using linear regression in Python
* Using ggplot in Python 
* Doing KMeans clustering 
* Doing PCA with scikit-learn 
* Doing ML classification prediction with scikit-learn 
* Doing ML Regression prediction with scikit-learn 
* Using Plotly for interactive Data Visualization

#### Data Collection Sources 

* *[Watch Video Lesson 9.1:  Data Collection of Social Media Data](https://www.safaribooksonline.com/videos/essential-machine-learning/9780135261118/9780135261118-EMLA_01_09_01)*

![Collection of Data](https://user-images.githubusercontent.com/58792/40758183-e64ba7c4-6440-11e8-97c5-c408e0bc321e.png)

**Twitter Code:**

https://github.com/noahgift/socialpowernba/blob/master/socialpower/sptwitter.py

**Wikipedia Code:**

https://github.com/noahgift/socialpowernba/blob/master/socialpower/spwikipedia.py

#### Import and merge DataFrames in Pandas

* *[Watch Video Lesson 9.2:  Import and merge DataFrames in Pandas](https://www.safaribooksonline.com/videos/essential-machine-learning/9780135261118/9780135261118-EMLA_01_09_02)*
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import statsmodels.api as sm
import statsmodels.formula.api as smf
import matplotlib.pyplot as plt
import seaborn as sns
color = sns.color_palette()
import warnings
warnings.filterwarnings("ignore")
# %matplotlib inline

attendance_df = pd.read_csv("https://raw.githubusercontent.com/noahgift/socialpowernba/master/data/nba_2017_attendance.csv");attendance_df.head()

endorsement_df = pd.read_csv("https://raw.githubusercontent.com/noahgift/socialpowernba/master/data/nba_2017_endorsements.csv");endorsement_df.head()

valuations_df = pd.read_csv("https://raw.githubusercontent.com/noahgift/socialpowernba/master/data/nba_2017_team_valuations.csv");valuations_df.head()

salary_df = pd.read_csv("https://raw.githubusercontent.com/noahgift/socialpowernba/master/data/nba_2017_salary.csv");salary_df.head()

pie_df = pd.read_csv("https://raw.githubusercontent.com/noahgift/socialpowernba/master/data/nba_2017_pie.csv");pie_df.head()

plus_minus_df = pd.read_csv("https://raw.githubusercontent.com/noahgift/socialpowernba/master/data/nba_2017_real_plus_minus.csv");plus_minus_df.head()

br_stats_df = pd.read_csv("https://raw.githubusercontent.com/noahgift/socialpowernba/master/data/nba_2017_br.csv");br_stats_df.head()

elo_df = pd.read_csv("https://raw.githubusercontent.com/noahgift/socialpowernba/master/data/nba_2017_elo.csv");elo_df.head()

"""### Exploratory Data Analysis (EDA)"""

attendance_valuation_df = attendance_df.merge(valuations_df, how="inner", on="TEAM")

attendance_valuation_df.head()

"""#### Understand correlation heatmaps and pairplots

Exploratory Data Analysis and Feature Engineering

* [Watch Video Lesson 9.3:  Understand correlation heatmaps and pairplots](https://www.safaribooksonline.com/videos/essential-machine-learning/9780135261118/9780135261118-EMLA_01_09_03)
"""

attendance_valuation_df.corr()

"""**Correlation Heatmap**"""

corr = attendance_valuation_df.corr()
sns.heatmap(corr, 
            xticklabels=corr.columns.values,
            yticklabels=corr.columns.values)

"""**Correlation DataFrame Output**"""

corr

"""**Creating a Pivot Table Based Heatmap in Seaborn**

A few patterns are detected:  Look at the *three highest valued signals*
"""

valuations = attendance_valuation_df.pivot("TEAM", "TOTAL_MILLIONS", "VALUE_MILLIONS")

plt.subplots(figsize=(20,15))
ax = plt.axes()
ax.set_title("NBA Team AVG Attendance vs Valuation in Millions:  2016-2017 Season")
sns.heatmap(valuations,linewidths=.5, annot=True, fmt='g')

"""#### Using linear regression in Python

There is a signal here, attendence and valuation do seem to be related, but residual values look non-uniform.

* *[Watch Video Lesson 9.4:  Use linear regression in Python](https://www.safaribooksonline.com/videos/essential-machine-learning/9780135261118/9780135261118-EMLA_01_09_04)*
"""

results = smf.ols('VALUE_MILLIONS ~TOTAL_MILLIONS', data=attendance_valuation_df).fit()

print(results.summary())

sns.residplot(y="VALUE_MILLIONS", x="TOTAL_MILLIONS", data=attendance_valuation_df)

attendance_valuation_predictions_df = attendance_valuation_df.copy()

attendance_valuation_predictions_df["predicted"] = results.predict()
attendance_valuation_predictions_df

"""#### Use seaborn lmplot to plot predicted vs actual values"""

sns.lmplot(x="predicted", y="VALUE_MILLIONS", data=attendance_valuation_predictions_df)

"""##### Generating a RMSE (Root Mean Squared Error Prediction)"""

import statsmodels
rmse = statsmodels.tools.eval_measures.rmse(attendance_valuation_predictions_df["predicted"], attendance_valuation_predictions_df["VALUE_MILLIONS"])
rmse

"""#### Adding ELO (Strength of Schedule Ranking to DataFrame)"""

attendance_valuation_elo_df = attendance_valuation_df.merge(elo_df, how="inner", on="TEAM")

attendance_valuation_elo_df.head()

corr_elo = attendance_valuation_elo_df.corr()
plt.subplots(figsize=(10,5))
ax = plt.axes()
ax.set_title("NBA Team Correlation Heatmap:  2016-2017 Season (ELO, AVG Attendance, VALUATION IN MILLIONS)")
sns.heatmap(corr_elo, 
            xticklabels=corr_elo.columns.values,
            yticklabels=corr_elo.columns.values)

corr_elo

ax = sns.lmplot(x="ELO", y="TOTAL_MILLIONS", data=attendance_valuation_elo_df, hue="CONF", size=6)
ax.set(xlabel='ELO Score', ylabel='TOTAL ATTENDANCE IN MILLIONS', title="NBA Team AVG Attendance vs ELO Ranking:  2016-2017 Season")

attendance_valuation_elo_df.groupby("CONF")["ELO"].median()

attendance_valuation_elo_df.groupby("CONF")["TOTAL_MILLIONS"].median()

results = smf.ols('TOTAL_MILLIONS ~ELO', data=attendance_valuation_elo_df).fit()

print(results.summary())

val_housing_win_df = pd.read_csv("https://raw.githubusercontent.com/noahgift/socialpowernba/master/data/nba_2017_att_val_elo_win_housing.csv");val_housing_win_df.head()

val_housing_win_df.columns

results = smf.ols('VALUE_MILLIONS ~COUNTY_POPULATION_MILLIONS+TOTAL_ATTENDANCE_MILLIONS+MEDIAN_HOME_PRICE_COUNTY_MILLIONS', data=val_housing_win_df).fit()
print(results.summary())

"""#### Using ggplot in Python

* *[Watch Vidoe Lesson 9.5:  Use ggplot in Python](https://www.safaribooksonline.com/videos/essential-machine-learning/9780135261118/9780135261118-EMLA_01_09_05)*
"""

!pip -q install ggplot

from ggplot import *
ggplot(val_housing_win_df, aes(x="TOTAL_ATTENDANCE_MILLIONS", y="VALUE_MILLIONS",
                               color="WINNING_SEASON")) + geom_point(size=400)

"""#### Use k-means clustering

**Unsupervised Machine Learning**

*   Unlabeled Data
*   "Discovers" Labels
*  Finds Hidden Patterns


**References:**



* *[Watch Video Lesson 9.6:  Use k-means clustering](https://www.safaribooksonline.com/videos/essential-machine-learning/9780135261118/9780135261118-EMLA_01_09_06)*
*  [Read Chapter 6:  Pragmatic AI: Social Power and Influence](https://www.safaribooksonline.com/library/view/pragmatic-ai-an/9780134863924/ch06.html#ch06) 
*   [Python Machine Learning](https://www.safaribooksonline.com/library/view/Python+Machine+Learning+-+Second+Edition/9781787125933/ch11.html#ch11lvl2sec114) Cluster and Silhoutte plot examples.



*NBA Season Faceted Cluster Plot *

![Discovering Clusters in the NBA](https://user-images.githubusercontent.com/58792/40759110-6a93a2f8-6445-11e8-980b-ecbb1a2cc029.png)

#### Data Preparation for Clustering

* Clustering on four columns:  Attendence, ELO, Valuation and Median Home Prices
* Scaling the data
"""

numerical_df = val_housing_win_df.loc[:,["TOTAL_ATTENDANCE_MILLIONS", "ELO", "VALUE_MILLIONS", "MEDIAN_HOME_PRICE_COUNTY_MILLIONS"]]

from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()
print(scaler.fit(numerical_df))
print(scaler.transform(numerical_df))

from sklearn.cluster import KMeans
k_means = KMeans(n_clusters=3)
kmeans = k_means.fit(scaler.transform(numerical_df))
val_housing_win_df['cluster'] = kmeans.labels_
val_housing_win_df.head()

# Yellowbrick method
from yellowbrick.cluster import KElbowVisualizer

k_means = KMeans()
visualizer = KElbowVisualizer(kmeans, k=(3,12))

visualizer.fit(scaler.transform(numerical_df))    # Fit the data to the visualizer
visualizer.poof()    # Draw/show/poof the data

"""Elbow method shows that 3 clusters is decent choice"""

distortions = []
for i in range(1, 11):
    km = KMeans(n_clusters=i,
            init='k-means++',
            n_init=10,
            max_iter=300,
            random_state=0)
    km.fit(scaler.transform(numerical_df))
    distortions.append(km.inertia_)
    
plt.plot(range(1,11), distortions, marker='o')
plt.xlabel('Number of clusters')
plt.ylabel('Distortion')
plt.title("Team Valuation Elbow Method Cluster Analysis")
plt.show()

"""##### Silhouette Plot"""

km = KMeans(n_clusters=3,
            init='k-means++',
            n_init=10,
            max_iter=300,
            random_state=0)
y_km = km.fit_predict(scaler.transform(numerical_df))

import numpy as np
from matplotlib import cm
from sklearn.metrics import silhouette_samples
cluster_labels = np.unique(y_km)
n_clusters = cluster_labels.shape[0]
silhouette_vals = silhouette_samples(scaler.transform(numerical_df),
                                     y_km,
                                     metric='euclidean')
y_ax_lower, y_ax_upper = 0, 0
yticks = []
for i, c in enumerate(cluster_labels):
    c_silhouette_vals = silhouette_vals[y_km == c]
    c_silhouette_vals.sort()
    y_ax_upper += len(c_silhouette_vals)
    color = cm.jet(float(i)/n_clusters)
    plt.barh(range(y_ax_lower, y_ax_upper), c_silhouette_vals, height=1.0, edgecolor='none',color=color)
    yticks.append((y_ax_lower + y_ax_upper)/2)
    y_ax_lower += len(c_silhouette_vals)
silhouette_avg = np.mean(silhouette_vals)
plt.axvline(silhouette_avg,
            color="red",
            linestyle="--")
plt.yticks(yticks, cluster_labels + 1)
plt.ylabel('Cluster')
plt.xlabel('Silhouette coefficient')
plt.title('Silhouette Plot Team Valuation')
plt.figure(figsize=(20,10))
plt.show()

"""##### Agglomerative clustering (Hierachial) vs KMeans clustering"""

f, (ax1, ax2) = plt.subplots(1, 2, figsize=(8, 3))
km = KMeans(n_clusters=2,
            random_state=0)
X = scaler.transform(numerical_df)
y_km = km.fit_predict(X)
ax1.scatter(X[y_km==0,0],
            X[y_km==0,1],
            c='lightblue',
            edgecolor='black',
            marker='o',
            s=40,
            label='cluster 1')
ax1.scatter(X[y_km==1,0],
            X[y_km==1,1],
            c='red',
            edgecolor='black',
            marker='s',
            s=40,
            label='cluster 2')
ax1.set_title('NBA Team K-means clustering')
from sklearn.cluster import AgglomerativeClustering

X = scaler.transform(numerical_df)
ac = AgglomerativeClustering(n_clusters=2,
                             affinity='euclidean',
                             linkage='complete')
y_ac = ac.fit_predict(X)
ax2.scatter(X[y_ac==0,0],
             X[y_ac==0,1],
             c='lightblue',
             edgecolor='black',
             marker='o',
            s=40,
            label='cluster 1')
ax2.scatter(X[y_ac==1,0],
            X[y_ac==1,1],
            c='red',
            edgecolor='black',
            marker='s',
            s=40,
            label='cluster 2')
ax2.set_title('NBA Team Agglomerative clustering')
plt.legend()
plt.show()

"""##### 3D Plot in R

![Valuation 3D Plot](https://user-images.githubusercontent.com/58792/36056809-7f87a266-0dbc-11e8-8877-9bb87905adbd.png)

Source Code:  https://github.com/noahgift/socialpowernba/blob/master/plot_team_cluster.R

#### Use PCA with sklearn

References:



1.  [ PCA sklearn](http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html)
"""

import pandas as pd
from sklearn.decomposition import PCA
pca = PCA(n_components=2)
pca.fit(numerical_df)
X = pca.transform(numerical_df)
print(f"Before PCA Reduction{numerical_df.shape}")
print(f"After PCA Reduction {X.shape}")

"""##### Simple Scatter Plot of Reduced Dimensions"""

plt.scatter(X[:, 0], X[:, 1])
plt.show()



"""#### Using yellowbrick road for Feature Ranking

![yb viz](https://user-images.githubusercontent.com/58792/48080128-d7b8d900-e1a1-11e8-8f8c-aba2473bfc81.png)

Another "road" to travel

```python
!pip install yellowbrick
from yellowbrick.features import Rank2D

visualizer = Rank2D(algorithm="pearson")
visualizer.fit_transform(val_housing_win_df.as)
visualizer.poof()```
"""

!pip -q install -U yellowbrick

from yellowbrick.features import Rank2D

visualizer = Rank2D(algorithm="pearson")
visualizer.fit_transform(numerical_df)
visualizer.poof()

"""#### Use ML classification prediction with scikit-learn

Create supervized classification prediction
"""

from sklearn.neighbors import KNeighborsClassifier
neigh = KNeighborsClassifier(n_neighbors=3)
neigh.fit?

"""#### ML Regression prediction with scikit-learn

Create supervized regression prediction
"""

from sklearn.neighbors import KNeighborsRegressor
neigh = KNeighborsRegressor(n_neighbors=2)
neigh.fit?

"""#### ML auto-sklearn

automated machine learning toolkit:  [automl drop-in replacement for scikit-learn estimator](http://automl.github.io/auto-sklearn/stable/)

Emerging trend is to automatically pick the right model using "Automl"


```

!pip install auto-sklearn

```



```python
import autosklearn.classification
```

#### Using Plotly for interactive Data Visualization

* *[Read related material covered in Chapter 10 of Pragmatic AI](https://www.safaribooksonline.com/library/view/pragmatic-ai-an/9780134863924/ch10.xhtml#ch10)*

* *[Watch Video Lesson 9:10:  Use Plotly for interactive data visualization](https://www.safaribooksonline.com/videos/essential-machine-learning/9780135261118/9780135261118-EMLA_01_09_10)*

Cell configuration to setup Plotly
Further documentation available from [Google on Plotly Colab Integration](https://colab.research.google.com/notebooks/charts.ipynb#scrollTo=YVhMPxwa-wmS)
"""

def configure_plotly_browser_state():
  import IPython
  display(IPython.core.display.HTML('''
        <script src="/static/components/requirejs/require.js"></script>
        <script>
          requirejs.config({
            paths: {
              base: '/static/base',
              plotly: 'https://cdn.plot.ly/plotly-1.5.1.min.js?noext',
            },
          });
        </script>
        '''))

"""##### Going Further with Real Estate Exploration"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
pd.set_option('display.float_format', lambda x: '%.3f' % x)
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import seaborn as sns; sns.set(color_codes=True)
from sklearn.cluster import KMeans
color = sns.color_palette()
from IPython.core.display import display, HTML
display(HTML("<style>.container { width:100% !important; }</style>"))
# %matplotlib inline

df = pd.read_csv("https://raw.githubusercontent.com/noahgift/real_estate_ml/master/data/Zip_Zhvi_SingleFamilyResidence.csv")

df.describe()

"""**Clean Up DataFrame**
Rename RegionName to ZipCode and Change Zip Code to String
"""

df.rename(columns={"RegionName":"ZipCode"}, inplace=True)
df["ZipCode"]=df["ZipCode"].map(lambda x: "{:.0f}".format(x))
df["RegionID"]=df["RegionID"].map(lambda x: "{:.0f}".format(x))
df.head()

median_prices = df.median()

median_prices.tail()

marin_df = df[df["CountyName"] == "Marin"].median()
sf_df = df[df["City"] == "San Francisco"].median()
palo_alto = df[df["City"] == "Palo Alto"].median()
df_comparison = pd.concat([marin_df, sf_df, palo_alto, median_prices], axis=1)
df_comparison.columns = ["Marin County", "San Francisco", "Palo Alto", "Median USA"]

"""**Plotly visualization**

[Shortcut view of plot if slow to load](http://nbviewer.jupyter.org/github/noahgift/real_estate_ml/blob/648361ce7392a0af29ce79780e6e5159c1a378e9/notebooks/explore_zillow_data_sets.ipynb)
"""

import cufflinks as cf
cf.go_offline()

from plotly.offline import init_notebook_mode
configure_plotly_browser_state()
init_notebook_mode(connected=False)


df_comparison.iplot(title="Bay Area Median Single Family Home Prices 1996-2017",
                    xTitle="Year",
                    yTitle="Sales Price",
                   #bestfit=True, bestfit_colors=["pink"],
                   #subplots=True,
                   shape=(4,1),
                    #subplot_titles=True,
                    fill=True,)

"""**Cluster on Size Rank and Price**"""

from sklearn.preprocessing import MinMaxScaler

columns_to_drop = ['RegionID', 'ZipCode', 'City', 'State', 'Metro', 'CountyName']
df_numerical = df.dropna()
df_numerical = df_numerical.drop(columns_to_drop, axis=1)

df_numerical.describe()

scaler = MinMaxScaler()
scaled_df = scaler.fit_transform(df_numerical)
kmeans = KMeans(n_clusters=3, random_state=0).fit(scaled_df)
print(len(kmeans.labels_))

cluster_df = df.copy(deep=True)
cluster_df.dropna(inplace=True)
cluster_df.describe()
cluster_df['cluster'] = kmeans.labels_
cluster_df['appreciation_ratio'] = round(cluster_df["2017-09"]/cluster_df["1996-04"],2)
cluster_df['CityZipCodeAppRatio'] = cluster_df['City'].map(str) + "-" + cluster_df['ZipCode'] + "-" + cluster_df["appreciation_ratio"].map(str)
cluster_df.head()

"""**Create a 3D Plot**

[Shortcut view of plot if slow to load](http://nbviewer.jupyter.org/github/noahgift/real_estate_ml/blob/648361ce7392a0af29ce79780e6e5159c1a378e9/notebooks/explore_zillow_data_sets.ipynb)
"""

import plotly.offline as py
import plotly.graph_objs as go

from plotly.offline import init_notebook_mode
configure_plotly_browser_state()
init_notebook_mode(connected=False)

trace1 = go.Scatter3d(
    x=cluster_df["appreciation_ratio"],
    y=cluster_df["1996-04"],
    z=cluster_df["2017-09"],
    mode='markers',
    text=cluster_df["CityZipCodeAppRatio"],
    marker=dict(
        size=12,
        color=cluster_df["cluster"],                # set color to an array/list of desired values
        colorscale='Viridis',   # choose a colorscale
        opacity=0.8
    )
)
#print(trace1)
data = [trace1]
layout = go.Layout(
    showlegend=False,
    title="30 Year History USA Real Estate Prices (Clusters Colored)",
    scene = dict(
        xaxis = dict(title='X: Appreciation Ratio'),
        yaxis = dict(title="Y:  1996 Prices"),
        zaxis = dict(title="Z:  2017 Prices"),
    ),
    width=1000,
    height=900,
)
fig = go.Figure(data=data, layout=layout)
py.iplot(fig, filename='3d-scatter-colorscale')

"""**NBA Player Endorsements Interactive Plotly Graph**

Reference:

*   https://plot.ly/~ngift/17/
"""

from plotly.offline import init_notebook_mode
configure_plotly_browser_state()
init_notebook_mode(connected=False)


import plotly.offline as py
from plotly.graph_objs import *
trace1 = {
  "x": ["LeBron James", "Kevin Durant", "James Harden", "Russell Westbrook", "Carmelo Anthony", "Dwyane Wade", "Chris Paul", "Derrick Rose", "Kyrie Irving", "Stephen Curry"], 
  "y": [55, 36, 20, 15, 8, 13, 8, 14, 13, 35], 
  "name": "Endorsements in Millions", 
  "type": "bar", 
  "uid": "df2707", 
  "xsrc": "ngift:16:53adec", 
  "ysrc": "ngift:16:0e0504"
}
trace2 = {
  "x": ["LeBron James", "Kevin Durant", "James Harden", "Russell Westbrook", "Carmelo Anthony", "Dwyane Wade", "Chris Paul", "Derrick Rose", "Kyrie Irving", "Stephen Curry"], 
  "y": [14.7, 6.29, 3.28, 4.28, 3.77, 4.67, 2.69, 3.27, 4.8, 17.57], 
  "name": "Wikipedia Pageviews", 
  "type": "bar", 
  "uid": "c9d073", 
  "xsrc": "ngift:16:53adec", 
  "ysrc": "ngift:16:fea27a"
}
trace3 = {
  "x": ["LeBron James", "Kevin Durant", "James Harden", "Russell Westbrook", "Carmelo Anthony", "Dwyane Wade", "Chris Paul", "Derrick Rose", "Kyrie Irving", "Stephen Curry"], 
  "y": [20.43, 12.24, 15.54, 17.34, 5.26, 2.52, 13.48, 1.17, 8.28, 18.8], 
  "name": "Wins Attributed to Player", 
  "type": "bar", 
  "uid": "cfe1ac", 
  "xsrc": "ngift:16:53adec", 
  "ysrc": "ngift:16:f3c87e"
}
trace4 = {
  "x": ["LeBron James", "Kevin Durant", "James Harden", "Russell Westbrook", "Carmelo Anthony", "Dwyane Wade", "Chris Paul", "Derrick Rose", "Kyrie Irving", "Stephen Curry"], 
  "y": [30.96, 26.5, 26.5, 26.5, 24.56, 23.2, 22.87, 21.32, 17.64, 12.11], 
  "name": "Salary in Millions", 
  "type": "bar", 
  "uid": "f83635", 
  "xsrc": "ngift:16:53adec", 
  "ysrc": "ngift:16:2cdf3e"
}
trace5 = {
  "x": ["LeBron James", "Kevin Durant", "James Harden", "Russell Westbrook", "Carmelo Anthony", "Dwyane Wade", "Chris Paul", "Derrick Rose", "Kyrie Irving", "Stephen Curry"], 
  "y": [5.53, 1.43, 0.97, 2.13, 0.72, 0.35, 0.83, 1.86, 1.54, 12.28], 
  "name": "Twitter Favorite Count/1000", 
  "type": "bar", 
  "uid": "9d1aad", 
  "xsrc": "ngift:16:53adec", 
  "ysrc": "ngift:16:191da9"
}
data = Data([trace1, trace2, trace3, trace4, trace5])
layout = {
  "barmode": "group", 
  "title": "2016-2017 NBA Season Endorsement and Social Power", 
  "xaxis": {
    "autorange": True, 
    "range": [-0.5, 9.5], 
    "type": "category"
  }, 
  "yaxis": {
    "autorange": True, 
    "range": [0, 57.8947368421], 
    "type": "linear"
  }
}
fig = Figure(data=data, layout=layout)
py.iplot(fig, filename='3d-scatter-colorscale')



"""## Feature Engineering for Machine Learning in the Real-World

![search for answer ](https://learning.oreilly.com/library/view/feature-engineering-for/9781491953235/assets/feml_0101.png) [1]



*   Full of False Starts
*   Frustrating Delays
*   Infrastructure Challenges
*   Laptop Problem (You laptop isn't production)
*   The real-world isn't Kaggle or MNIST

*Core Aspects of Machine Learning are*

**Models**

* mathmetical model describes relationship between data


**Features**

* numeric representation of raw data

### Feature Engineering

![Feature Engineering](https://learning.oreilly.com/library/view/feature-engineering-for/9781491953235/assets/feml_0102.png)

![Technical Debt](https://user-images.githubusercontent.com/58792/60345103-3bd51b00-9986-11e9-84c7-40a25fd2bf14.png)

### ML Technical Debt Takeaways

**Real World Machine Learning System**

![Technical Debt Takeaways](https://user-images.githubusercontent.com/58792/60346009-4395bf00-9988-11e9-8085-fb7a6e50ec24.jpeg)

*   **5%** of the code in an ML system is ML code [2]
*   (at least) **95%** is glue code [2]

![ML Engineer](https://d3ansictanv2wj.cloudfront.net/Figure3-7c5de9f92f3406e23d76e6bff3f89818.png) [3]


"A common starting point is 2-3 data engineers for every data scientist. For some organizations with more complex data engineering requirements, this can be **4-5 data engineers per data scientist.** ""

![Five to One](https://user-images.githubusercontent.com/58792/49955060-c062bf00-feb6-11e8-8204-79d629ab2862.png)

### Feature Engineering in Practice

#### Permutation Importance

* [What features have the biggest impact on predictions?](https://www.kaggle.com/dansbecker/permutation-importance)
"""

fifa = "https://raw.githubusercontent.com/noahgift/socialpowernba/master/FIFA%202018%20Statistics.csv"
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier

data = pd.read_csv(fifa)
y = (data['Man of the Match'] == "Yes")  # Convert from string "Yes"/"No" to binary
feature_names = [i for i in data.columns if data[i].dtype in [np.int64]]
X = data[feature_names]
train_X, val_X, train_y, val_y = train_test_split(X, y, random_state=1)
my_model = RandomForestClassifier(random_state=0).fit(train_X, train_y)

"""eli5 feature importance"""

!pip install -q eli5

import eli5
from eli5.sklearn import PermutationImportance

perm = PermutationImportance(my_model, random_state=1).fit(val_X, val_y)
eli5.show_weights(perm, feature_names = val_X.columns.tolist())

"""### Scaling Data

Scaling

* In some cases (e.g., linear regression), you don't need to worry about the scale of your features.
  * Linear regression has a closed-form algebraic solution.
* However, optimization usually follows an iterative procedure to refine feature weight estimates.
   * In these cases, your features should be roughly in the same scale.
   * **To be safe, apply mean and variance scaling to all feature values** before trying to fit model.
   * Mean scaling: Find the mean value of a particular feature and subtract the mean from each feature.
   * Variance scaling: Divide each feature by its standard deviation.

**DEMO: Example NBA Team Valuation**
"""

import pandas as pd

team_data = "https://raw.githubusercontent.com/noahgift/socialpowernba/master/data/nba_2017_att_val_elo_win_housing.csv"
val_housing_win_df = pd.read_csv(team_data)
val_housing_win_df.head()

"""#### Data Preparation for Clustering

* Clustering on four columns:  Attendence, ELO, Valuation and Median Home Prices
* Scaling the data
"""

numerical_df = val_housing_win_df.loc[:,["TOTAL_ATTENDANCE_MILLIONS", "ELO", "VALUE_MILLIONS", "MEDIAN_HOME_PRICE_COUNTY_MILLIONS"]]

from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()
print(scaler.fit(numerical_df))
print(scaler.transform(numerical_df))

from sklearn.cluster import KMeans
k_means = KMeans(n_clusters=3)
kmeans = k_means.fit(scaler.transform(numerical_df))
val_housing_win_df['cluster'] = kmeans.labels_
val_housing_win_df.head()

"""##### 3D Plot in R

![Valuation 3D Plot](https://user-images.githubusercontent.com/58792/36056809-7f87a266-0dbc-11e8-8877-9bb87905adbd.png)

Source Code:  https://github.com/noahgift/socialpowernba/blob/master/plot_team_cluster.R
"""



"""### 3D Plot in Plotly"""

def enable_plotly_in_cell():
  import IPython
  from plotly.offline import init_notebook_mode
  display(IPython.core.display.HTML('''
        <script src="/static/components/requirejs/require.js"></script>
  '''))
  init_notebook_mode(connected=False)

import plotly.offline as py
import plotly.graph_objs as go

from plotly.offline import init_notebook_mode
enable_plotly_in_cell()
init_notebook_mode(connected=False)

trace1 = go.Scatter3d(
    x=val_housing_win_df["VALUE_MILLIONS"],
    y=val_housing_win_df["ELO"],
    z=val_housing_win_df["MEDIAN_HOME_PRICE_COUNTY_MILLIONS"],
    mode='markers',
    text=val_housing_win_df["TEAM"],
    marker=dict(
        size=12,
        color=val_housing_win_df["cluster"],                # set color to an array/list of desired values
        colorscale='Viridis',   # choose a colorscale
        opacity=0.8
    )
)
#print(trace1)
data = [trace1]
layout = go.Layout(
    showlegend=False,
    title="Valuations NBA Teams (Clusters Colored)",
    scene = dict(
        xaxis = dict(title='X: VALUE_MILLIONS'),
        yaxis = dict(title="Y:  ELO"),
        zaxis = dict(title="Z:  MEDIAN_HOME_PRICE_COUNTY_MILLIONS"),
    ),
    width=1000,
    height=900,
)
fig = go.Figure(data=data, layout=layout)
py.iplot(fig, filename='3d-scatter-colorscale')

"""### Bias

**There are many different sources of bias:**

* Unbalanced ratio of positive to negative examples
* This may cause the classifier to always pick the majority class.
* Mismatched sources
* Training procedure is focused on the training data provided.
* Algorithm won't work well with fundamentally different data.

### Key Takeaways

**Key Takeaways**

* If you have around 1,000 to 10,000 training examples, it’s worth trying more complex algorithms (e.g., SVMs, random forests).
* The more data you have, the harder it will be to beat logistic regression.
* Neural nets can discover useful features in complex inputs (e.g., images, speech).

**sklearn ml map**


![sklearn ml map](https://scikit-learn.org/stable/_static/ml_map.png)

### Case Studies

#### Predict a Winning Team
"""

import pandas as pd

player_data = "https://raw.githubusercontent.com/noahgift/socialpowernba/master/data/nba_2017_players_with_salary_wiki_twitter.csv"
df = pd.read_csv(player_data)
df.head()

"""#### Feature Engineering:  Create a new Feature...Winning Season"""

def winning_season(wins):
  
  if wins > 42:
      return 1
  return 0

df["winning_season"] = df["W"].apply(winning_season)

import seaborn as sns
sns.scatterplot(x="W", y="AGE", hue="winning_season", data=df)

"""#### Predict Winning Season"""

df2 = df[["AGE", "POINTS", "SALARY_MILLIONS", "PAGEVIEWS", "TWITTER_FAVORITE_COUNT","winning_season", "TOV"]]
df = df2.dropna()
target = df["winning_season"]
features = df[["AGE", "POINTS","SALARY_MILLIONS", "PAGEVIEWS", "TWITTER_FAVORITE_COUNT", "TOV"]]
classes = ["winning", "losing"]

df2.shape

from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(features, target, test_size=0.25, random_state=0)

x_train.shape

"""Use Logistic Regression"""

from sklearn.linear_model import LogisticRegression

model = LogisticRegression()
model.fit(x_train, y_train)

"""**Confusion Matrix**

* precision:  “for all instances classified positive, what percent was correct?”
* recall: "for all instances that were actually positive, what percent was classified correctly?""
"""

from yellowbrick.classifier import ClassificationReport
visualizer = ClassificationReport(model, classes=classes, support=True)
visualizer.fit(x_train, y_train)  # Fit the visualizer and the model
visualizer.score(x_test, y_test)  # Evaluate the model on the test data
g = visualizer.poof()             # Draw/show/poof the data

"""#### Feature Ranking with Yellowbrick"""

from yellowbrick.features import Rank2D

visualizer = Rank2D(algorithm="pearson")
visualizer.fit_transform(df)
visualizer.poof()

"""#### Explain Model with shap"""

!pip install -q  shap

import xgboost
import shap

model_xgboost = xgboost.train({"learning_rate": 0.01}, xgboost.DMatrix(x_train, label=y_train), 100)

# load JS visualization code to notebook
shap.initjs()

# explain the model's predictions using SHAP values
# (same syntax works for LightGBM, CatBoost, and scikit-learn models)
explainer = shap.TreeExplainer(model_xgboost)
shap_values = explainer.shap_values(features)

# visualize the first prediction's explanation (use matplotlib=True to avoid Javascript)
shap.force_plot(explainer.expected_value, shap_values[0,:], features.iloc[0,:])

"""Explain all of the features"""

shap.summary_plot(shap_values, features)

"""bar plot"""

shap.summary_plot(shap_values, features, plot_type="bar")

"""#### ELI5 (Using Logistic Regression)"""

!pip install -q eli5

import eli5
from eli5.sklearn import PermutationImportance

perm = PermutationImportance(model, random_state=1).fit(x_train, y_train)
eli5.show_weights(perm, feature_names = features.columns.tolist())

"""### H20AutoML

#### Demo (Warning takes about 10 minutes to run)

Install H20
"""

!pip install -q h2o

import h2o
from h2o.automl import H2OAutoML

h2o.init()

# Import a sample binary outcome train/test set into H2O
train = h2o.import_file("https://s3.amazonaws.com/erin-data/higgs/higgs_train_10k.csv")
test = h2o.import_file("https://s3.amazonaws.com/erin-data/higgs/higgs_test_5k.csv")

# Identify predictors and response
x = train.columns
y = "response"
x.remove(y)

# For binary classification, response should be a factor
train[y] = train[y].asfactor()
test[y] = test[y].asfactor()

# Run AutoML for 20 base models (limited to 1 hour max runtime by default)
aml = H2OAutoML(max_models=20, seed=1)
aml.train(x=x, y=y, training_frame=train)

# View the AutoML Leaderboard
lb = aml.leaderboard
lb.head(rows=lb.nrows)  # Print all rows instead of default (10 rows)

aml.predict?

"""## References

*   [Pragmatic AI: An Introduction to Cloud-Based Machine Learning-Physical Book](http://www.informit.com/store/pragmatic-ai-an-introduction-to-cloud-based-machine-9780134863917)
*   [Pragmatic AI: Pragmatic AI: An Introduction to Cloud-Based Machine Learning-SafariOnline Book](https://www.safaribooksonline.com/library/view/pragmatic-ai-an/9780134863924/)
*  [Essential Machine Learning and AI with Python and Jupyter Notebook-Video-SafariOnline](https://www.safaribooksonline.com/videos/essential-machine-learning/9780135261118)
* [Essential Machine Learning and AI with Python and Jupyter Notebook- Purchase Video](http://www.informit.com/store/essential-machine-learning-and-ai-with-python-and-jupyter-9780135261095)
* [AWS Certified Machine Learning-Speciality](https://learning.oreilly.com/videos/aws-certified-machine/9780135556597)
*   [noahgift.com](https://noahgift.com/)
*   [paiml.com](https://paiml.com/)
*   [Kaggle Social Power NBA](https://www.kaggle.com/noahgift/social-power-nba)
*  [Pandas Internals Use 5-10x memory of Dataset](http://wesmckinney.com/blog/apache-arrow-pandas-internals/)
* [JSON schema python](https://pypi.org/project/jsonschema/)

## FAQ

### Q:  How do I pick the correct number of clusters in unsupervized machine learning?

**Answer:  **  There isn't one correct answer, because it can depend on interpretation.  Two common methods to help decide are elbow plot and [silhouette plot](http://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html)
"""

